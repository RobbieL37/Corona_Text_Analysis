{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cdb620e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "import plotly.express as px\n",
    "import scipy.stats\n",
    "import math\n",
    "import statistics\n",
    "import string \n",
    "import re\n",
    "from collections import defaultdict\n",
    "from gensim import corpora, models\n",
    "from nltk.text import Text\n",
    "# ----------------------------------------\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "# ----------------------------------------\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf03e0",
   "metadata": {},
   "source": [
    "8 data (x) \n",
    "ranking negative(1-3) positive(4,5) (x) \n",
    "single words + Bi-grame + Tri-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98423e75",
   "metadata": {},
   "source": [
    "### Import all the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c095322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data importing \n",
    "lowes_6026 = pd.read_csv('C:\\\\Users\\\\lxf12\\\\Desktop\\\\Folder of folders\\\\Corona New project\\\\Lowes_Homedepot_Reviews\\\\lowes.com_3286026.csv')\n",
    "lowes_0314 = pd.read_csv('C:\\\\Users\\\\lxf12\\\\Desktop\\\\Folder of folders\\\\Corona New project\\\\Lowes_Homedepot_Reviews\\\\lowes.com_3380314.csv')\n",
    "lowes_0511 = pd.read_csv('C:\\\\Users\\\\lxf12\\\\Desktop\\\\Folder of folders\\\\Corona New project\\\\Lowes_Homedepot_Reviews\\\\lowes.com_50280511.csv') \n",
    "lowes_6162 = pd.read_csv('C:\\\\Users\\\\lxf12\\\\Desktop\\\\Folder of folders\\\\Corona New project\\\\Lowes_Homedepot_Reviews\\\\lowes.com_1003066162.csv')\n",
    "lowes_6209 = pd.read_csv('C:\\\\Users\\\\lxf12\\\\Desktop\\\\Folder of folders\\\\Corona New project\\\\Lowes_Homedepot_Reviews\\\\lowes.com_5000026209.csv')\n",
    "lowes_9855 = pd.read_csv('C:\\\\Users\\\\lxf12\\\\Desktop\\\\Folder of folders\\\\Corona New project\\\\Lowes_Homedepot_Reviews\\\\lowes.com_5001899855.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4acbbcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data importing \n",
    "homedepot_4796 = pd.read_csv('C:\\\\Users\\\\lxf12\\\\Desktop\\\\Folder of folders\\\\Corona New project\\\\Lowes_Homedepot_Reviews\\\\homedepot.com_204074796.csv')\n",
    "homedepot_7071 = pd.read_csv('C:\\\\Users\\\\lxf12\\\\Desktop\\\\Folder of folders\\\\Corona New project\\\\Lowes_Homedepot_Reviews\\\\homedepot.com_319247071.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0698ea5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lxf12\\AppData\\Local\\Temp\\ipykernel_24240\\779606820.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "# set display option to max to see all information in review column\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "870c0b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lowes_0314.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8ec600",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowes_6026 has null rows amount: 30\n",
      "after cleaning, lowes_6026 has null rows amount: 0\n",
      "lowes_0314 has null rows amount: 10\n",
      "after cleaning, lowes_0314 has null rows amount: 0\n",
      "lowes_0511 has null rows amount: 15\n",
      "after cleaning, lowes_0511 has null rows amount: 0\n",
      "lowes_6162 has null rows amount: 297\n",
      "after cleaning, lowes_6162 has null rows amount: 0\n",
      "lowes_6209 has null rows amount: 76\n",
      "after cleaning, lowes_6209 has null rows amount: 0\n",
      "lowes_9855 has null rows amount: 170\n",
      "after cleaning, lowes_9855 has null rows amount: 0\n",
      "homedepot_4796 has null rows amount: 186\n",
      "after cleaning, homedepot_4796 has null rows amount: 0\n",
      "homedepot_7071 has null rows amount: 9\n",
      "after cleaning, homedepot_7071 has null rows amount: 0\n"
     ]
    }
   ],
   "source": [
    "# drop null value\n",
    "datalist = [lowes_6026, lowes_0314, lowes_0511, lowes_6162,\n",
    "                    lowes_6209, lowes_9855, \n",
    "                     homedepot_4796, homedepot_7071]\n",
    "namelist = ['lowes_6026', 'lowes_0314', 'lowes_0511', 'lowes_6162',\n",
    "                    'lowes_6209', 'lowes_9855', \n",
    "                     'homedepot_4796', 'homedepot_7071']\n",
    "n = 0 \n",
    "\n",
    "for i in datalist: \n",
    "    # drop rows with Not a Number (NaN) and None values\n",
    "    name = namelist[n]\n",
    "    n += 1\n",
    "    print(name + ' has null rows amount: '+ str(i['Review'].isna().sum()))\n",
    "    i.dropna(subset=['Review'], inplace=True)\n",
    "    print('after cleaning, ' + name + ' has null rows amount: ' \n",
    "          + str(i['Review'].isna().sum()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f8204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowes_9855.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1f47119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datalist[0].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ea1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76f2874b",
   "metadata": {},
   "source": [
    "### Separate Negative and Positive Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "846166b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select negative(0) or positive ranking(1): 0\n"
     ]
    }
   ],
   "source": [
    "# 8 data x 2 categories \n",
    "try:\n",
    "    user_select = int(input(\"Please select negative(0) or positive ranking(1): \"))\n",
    "except: \n",
    "    print(\"Your input is not correct. Please input again according the tips.\")\n",
    "\n",
    "if user_select == 0:\n",
    "    # get reviews that have given 1, 2, and 3 stars (negative)\n",
    "    lowes_6026 = lowes_6026.loc[lowes_6026['Stars'] <= 3]\n",
    "    lowes_0314 = lowes_0314.loc[lowes_0314['Stars'] <= 3]\n",
    "    lowes_0511 = lowes_0511.loc[lowes_0511['Stars'] <= 3]\n",
    "    lowes_6162 = lowes_6162.loc[lowes_6162['Stars'] <= 3]\n",
    "    lowes_6209 = lowes_6209.loc[lowes_6209['Stars'] <= 3]\n",
    "    lowes_9855 = lowes_9855.loc[lowes_9855['Stars'] <= 3]\n",
    "    # get reviews that have given 1, 2, and 3 stars (negative)\n",
    "    homedepot_4796 = homedepot_4796.loc[homedepot_4796['Stars'] <= 3]\n",
    "    homedepot_7071 = homedepot_7071.loc[homedepot_7071['Stars'] <= 3]\n",
    "elif user_select == 1:\n",
    "    # get reviews that have given 4 and 5 stars (positive)\n",
    "    lowes_6026 = lowes_6026.loc[lowes_6026['Stars'] > 3]\n",
    "    lowes_0314 = lowes_0314.loc[lowes_0314['Stars'] > 3]\n",
    "    lowes_0511 = lowes_0511.loc[lowes_0511['Stars'] > 3]\n",
    "    lowes_6162 = lowes_6162.loc[lowes_6162['Stars'] > 3]\n",
    "    lowes_6209 = lowes_6209.loc[lowes_6209['Stars'] > 3]\n",
    "    lowes_9855 = lowes_9855.loc[lowes_9855['Stars'] > 3]\n",
    "    # get reviews that have given 4 and 5 stars (positive)\n",
    "    homedepot_4796 = homedepot_4796.loc[homedepot_4796['Stars'] > 3]\n",
    "    homedepot_7071 = homedepot_7071.loc[homedepot_7071['Stars'] > 3]\n",
    "else: \n",
    "    print(\"Your input is not correct. Please input again according the tips.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3593942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = [lowes_6026, lowes_0314, lowes_0511, lowes_6162,\n",
    "                    lowes_6209, lowes_9855, \n",
    "                     homedepot_4796, homedepot_7071]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec1d7292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lowes_0314.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac48f2",
   "metadata": {},
   "source": [
    "### Lower case for all words & Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c535bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download package punkt for word_tokenize\n",
    "# nltk.download('punkt')\n",
    "# download package for stopwords\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6b0a1-9e6a-4704-a4d1-eb2acdea72fd",
   "metadata": {},
   "source": [
    "### Store them to text list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "121d75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before the cleaning \n",
    "text_01 = lowes_6026['Review'].to_string(index=False)\n",
    "# text_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee2eebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a translation table with all punctuation characters mapped to None\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "text = [0] * 8\n",
    "n = 0 \n",
    "\n",
    "for i in datalist:\n",
    "    # convert pandas series data to string\n",
    "    text[n] = i['Review'].to_string(index=False)\n",
    "    # convert all characters to lowercase\n",
    "    text[n] = text[n].lower()\n",
    "    # remove punctuations using RegexpTokenizer\n",
    "    text[n] = text[n].translate(translator)\n",
    "    # Use regex to remove all numbers\n",
    "    text[n] = re.sub(r'\\d+', '', text[n])\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6edb3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result has been single words \n",
    "# text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0351e2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4a7e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renew the datalist --> can't be deleted \n",
    "datalist = [lowes_6026, lowes_0314, lowes_0511, lowes_6162,\n",
    "                    lowes_6209, lowes_9855, \n",
    "                     homedepot_4796, homedepot_7071]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1260f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5683fe8",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee2b5f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lxf12\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # customize stop_words by including a custom list of words\n",
    "    custom_stop_words = ['toilet']\n",
    "#     custom_stop_words = ['new','toilet', 'factory', 'flush', 'sku', 'photos', 'stay', \n",
    "#                                      'lid', 'good', 'great', 'handle', 'top', 'high', 'urine']\n",
    "    stop_words.update(custom_stop_words)\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4212bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    text[i] = remove_stopwords(text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af6afe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renew the datalist --> can't be deleted \n",
    "datalist = [lowes_6026, lowes_0314, lowes_0511, lowes_6162,\n",
    "                    lowes_6209, lowes_9855, \n",
    "                     homedepot_4796, homedepot_7071]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4995b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70c58a46-ca67-4646-9f10-e7294cfcb11b",
   "metadata": {},
   "source": [
    "### Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d359be9d-8bf8-4e58-9943-7610f3964cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations using RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = [0] * 8\n",
    "for i in range(8):\n",
    "    tokens[i] = tokenizer.tokenize(text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19e46dbc-4f87-4923-8acf-fb5b099372ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d8c66-7b03-46c4-ab8b-c88cb0fbe2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29079591",
   "metadata": {},
   "source": [
    "### Bi-gram and Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6eb6a7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many words do you want to aggregate? (1-8)4\n"
     ]
    }
   ],
   "source": [
    "# N-gram generation\n",
    "N = int(input('How many words do you want to aggregate? (1-8)'))\n",
    "\n",
    "trigrams = [0] * 8\n",
    "for i in range(8):\n",
    "    trigrams[i] = list(ngrams(tokens[i], N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adb3983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect the three words together \n",
    "words = [0] * 8\n",
    "for i in range(8):\n",
    "    words[i] = [' '.join(tr) for tr in trigrams[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70bccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fcadce1",
   "metadata": {},
   "source": [
    "### Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7570342c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which one do you want to look at? (1-8)1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('american', 'standard', 'customer', 'service'), 3),\n",
       " (('needs', 'cleaned', 'every', 'day'), 2),\n",
       " (('american', 'standard', 'mainstream', 'model'), 2),\n",
       " (('one', 'market', 'flow', 'master'), 2),\n",
       " (('small', 'amount', 'water', 'enters'), 2),\n",
       " (('amount', 'water', 'enters', 'bowl'), 2),\n",
       " (('water', 'enters', 'bowl', 'enough'), 2),\n",
       " (('like', 'facts', 'uses', 'little'), 1),\n",
       " (('facts', 'uses', 'little', 'water'), 1),\n",
       " (('uses', 'little', 'water', 'chair'), 1)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the data \n",
    "n = int(input('Which one do you want to look at? (1-8)'))\n",
    "\n",
    "word_frequency = FreqDist(trigrams[n-1])\n",
    "\n",
    "# Identify co-occurring trigrams\n",
    "cooccurring_trigrams = []\n",
    "for trigram in word_frequency:\n",
    "    if word_frequency[trigram] > 1:\n",
    "        cooccurring_trigrams.append(trigram)\n",
    "\n",
    "word_frequency.most_common(10)\n",
    "\n",
    "# word_frequency.plot(30,cumulative=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa8d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfdfaa26",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "#### Latent Dirichlet Allocation (LDA), which is a popular unsupervised learning algorithm used for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55791e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11cc6301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which one do you want to look at? (1-8)1\n",
      "Topic 0: 0.001*\"i m happy\" + 0.001*\"hair line crack\" + 0.001*\"water keeps running\" + 0.001*\"flush doesnt clean\"\n",
      "Topic 1: 0.001*\"flush valve assembly\" + 0.001*\"cheap plastic seat\" + 0.001*\"bought two toilets\" + 0.001*\"adjust water level\"\n",
      "Topic 2: 0.001*\"six eight times\" + 0.001*\"flushed several times\" + 0.001*\"nasty rusty bolts\" + 0.001*\"small rubber washer\"\n"
     ]
    }
   ],
   "source": [
    "# Step 1: select the data \n",
    "n = int(input('Which one do you want to look at? (1-8)'))\n",
    "\n",
    "# Step 2: Create a dictionary of words and convert the text into a bag-of-words format\n",
    "dictionary = corpora.Dictionary([words[n-1]])\n",
    "corpus = [dictionary.doc2bow([word]) for word in words[n-1]]\n",
    "\n",
    "# Step 3: Apply LDA to the corpus to identify topics in the text\n",
    "lda_model = models.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=10)\n",
    "\n",
    "# Step 4: Print the topics and the top words associated with each topic\n",
    "for i, topic in lda_model.show_topics(formatted=True, num_topics=3, num_words=4):\n",
    "    print(\"Topic {}: {}\".format(i, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ce9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51c3d2db",
   "metadata": {},
   "source": [
    "### Text Summarization\n",
    "#### In summary, while topic modeling aims to uncover the underlying themes and topics in a large corpus of text, text summarization aims to provide a condensed version of a longer text, capturing the essential information and key points.\n",
    "#### Two different approaches are used for Text Summarization\n",
    "1. Extractive Summarization: In Extractive Summarization, we identify essential phrases or sentences from the original text and extract only these phrases from the text. These extracted sentences would be the summary.\n",
    "2. Abstractive Summarization: We work on generating new sentences from the original text in the Abstractive Summarization approach. The abstractive method contrasts the approach described above, and the sentences generated through this approach might not even be present in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2260ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c95e207-0da1-4354-a09e-8232fd1c05e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which one do you want to look at? (1-8)1\n"
     ]
    }
   ],
   "source": [
    "# select the data \n",
    "n = int(input('Which one do you want to look at? (1-8)'))\n",
    "result_text_1 = datalist[n-1]['Review'].to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5915b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(result_text_1)\n",
    "freqTable = dict()\n",
    "for word in words:\n",
    "    word = word.lower()\n",
    "    if word in stopWords:\n",
    "        continue\n",
    "    if word in freqTable:\n",
    "        freqTable[word] += 1\n",
    "    else:\n",
    "        freqTable[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc830825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(result_text_1)\n",
    "sentenceValue = dict()\n",
    "for sentence in sentences:\n",
    "    for word, freq in freqTable.items():\n",
    "        if word in sentence.lower():\n",
    "            if sentence in sentenceValue:\n",
    "                sentenceValue[sentence] += freq\n",
    "            else:\n",
    "                sentenceValue[sentence] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50d62ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumValues = 0\n",
    "for sentence in sentenceValue:\n",
    "    sumValues += sentenceValue[sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f676513",
   "metadata": {},
   "outputs": [],
   "source": [
    "average = int(sumValues / len(sentenceValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e24c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing sentences into our summary.\n",
    "summary = ''\n",
    "for sentence in sentences:\n",
    "    if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.8 * average)):\n",
    "        summary += \" \" + sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ade3440b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After removing the old toilet and setting the base and tank I opened the new box only to find the tank lid cracked, this is poorly put together and hard to get it swallow anything                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "If you have bought this toilet you realize you must flush multiple times when going number 2 so it doesn’t clog up. the worst toilet I have ever had it flush itself tried to fix it with new parts, still flush and I am not talking about leaking.it completely empties the tank                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "If you have it spend the money on the better toilet 3 days after install had to buy new insides because would not stop running and sweating.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002f2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3af3621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# from string import punctuation\n",
    "# from heapq import nlargest\n",
    "# from spacy.lang.en.examples import sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3898267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize(text, per):\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "#     doc= nlp(text)\n",
    "#     tokens=[token.text for token in doc]\n",
    "#     word_frequencies={}\n",
    "#     for word in doc:\n",
    "#         if word.text.lower() not in list(STOP_WORDS):\n",
    "#             if word.text.lower() not in punctuation:\n",
    "#                 if word.text not in word_frequencies.keys():\n",
    "#                     word_frequencies[word.text] = 1\n",
    "#                 else:\n",
    "#                     word_frequencies[word.text] += 1\n",
    "#     max_frequency=max(word_frequencies.values())\n",
    "#     for word in word_frequencies.keys():\n",
    "#         word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "#     sentence_tokens= [sent for sent in doc.sents]\n",
    "#     sentence_scores = {}\n",
    "#     for sent in sentence_tokens:\n",
    "#         for word in sent:\n",
    "#             if word.text.lower() in word_frequencies.keys():\n",
    "#                 if sent not in sentence_scores.keys():                            \n",
    "#                     sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "#                 else:\n",
    "#                     sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "#     select_length=int(len(sentence_tokens)*per)\n",
    "#     summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "#     final_summary=[word.text for word in summary]\n",
    "#     summary=''.join(final_summary)\n",
    "#     return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7d3fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datalist = [lowes_6026, lowes_0314, lowes_0511, lowes_6162,\n",
    "#                     lowes_6209, lowes_9855, \n",
    "#                      homedepot_4796, homedepot_7071]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cef9e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_6026 = lowes_6026['Review'].to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c659586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(summarize(result_text_1, 0.03))\n",
    "# # per is the percentage (0 to 1) of sentences you want to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8473b2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b82d5a8",
   "metadata": {},
   "source": [
    "### Context Analysis\n",
    "#### Check out the context of words having highest frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7366a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which one do you want to look at? (1-8)1\n"
     ]
    }
   ],
   "source": [
    "# select the data \n",
    "n = int(input('Which one do you want to look at? (1-8)'))\n",
    "result_text_1 = text[n-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e20dea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ADJ: 1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pos and lemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemm = lemmatizer.lemmatize(result_text_1)\n",
    "pos_tag_1 = pos_tag(word_tokenize(lemm))\n",
    "# pos_tag_1\n",
    "\n",
    "### select Adjective or Noun \n",
    "result_adj_1 = []\n",
    "for token, tag in pos_tag_1:\n",
    "    if tag[0:1].lower() == \"j\":   ### j represents Adj. n represents noun\n",
    "        result_adj_1.append(token)\n",
    "print(f\"Total number of ADJ: {len(result_adj_1)}\\n\")\n",
    "# print(result_adj_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87ce7736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 most common Adjective. word appear in review is: [('flush', 70), ('american', 36), ('new', 33), ('good', 32), ('old', 23), ('small', 19), ('great', 18), ('little', 17), ('much', 15), ('bad', 15)]\n"
     ]
    }
   ],
   "source": [
    "# see each Adjective frequence\n",
    "ADJ_f_1 = FreqDist(result_adj_1)\n",
    "# ADJ_f_1\n",
    "print(f\"The top 10 most common Adjective. word appear in review is: {ADJ_f_1.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "143a5a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of noun: 2804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pos and lemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemm = lemmatizer.lemmatize(result_text_1)\n",
    "pos_tag_1 = pos_tag(word_tokenize(lemm))\n",
    "# pos_tag_1\n",
    "\n",
    "### select Adjective or Noun \n",
    "result_adj_1 = []\n",
    "for token, tag in pos_tag_1:\n",
    "    if tag[0:1].lower() == \"n\":   ### j represents Adj. n represents noun\n",
    "        result_adj_1.append(token)\n",
    "print(f\"Total number of noun: {len(result_adj_1)}\\n\")\n",
    "# print(result_adj_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a09cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 most common Noun. word appear in review is: [('water', 140), ('tank', 62), ('seat', 56), ('bowl', 49), ('flush', 47), ('toilets', 40), ('standard', 39), ('time', 32), ('parts', 26), ('level', 26)]\n"
     ]
    }
   ],
   "source": [
    "# see each noun frequence\n",
    "noun_f_1 = FreqDist(result_adj_1)\n",
    "print(f\"The top 10 most common Noun. word appear in review is: {noun_f_1.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e7ee35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of verb: 1566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pos and lemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemm = lemmatizer.lemmatize(result_text_1)\n",
    "pos_tag_1 = pos_tag(word_tokenize(lemm))\n",
    "# pos_tag_1\n",
    "\n",
    "### select Adjective or Noun \n",
    "result_adj_1 = []\n",
    "for token, tag in pos_tag_1:\n",
    "    if tag[0:1].lower() == \"v\":   ### j represents Adj. n represents noun\n",
    "        result_adj_1.append(token)\n",
    "print(f\"Total number of verb: {len(result_adj_1)}\\n\")\n",
    "# print(result_adj_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0a67cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 most common Verb. word appear in review is: [('installed', 42), ('replace', 32), ('purchased', 32), ('bought', 29), ('buy', 27), ('get', 26), ('flushing', 25), ('running', 23), ('said', 22), ('went', 19)]\n"
     ]
    }
   ],
   "source": [
    "# see each noun frequence\n",
    "noun_f_1 = FreqDist(result_adj_1)\n",
    "print(f\"The top 10 most common Verb. word appear in review is: {noun_f_1.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce7284b",
   "metadata": {},
   "source": [
    "#### concordance function: to locate the key word and get the context\n",
    "https://www.nltk.org/howto/concordance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3e7eed2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the samples of the water:\n",
      "nt fine installed became problem old years plus base went rather easily water tank became problem would seat properly developed leaks wabbled afraid \n",
      "l guess trouble flushing one time far better old one course still brush water ring like old one wife likes taller seat height still works well ok uni\n",
      "cannot begin express years discomfort disagreements family endured whim water sense thats deceptive code low flow ive quietly tried defend purchase t\n",
      "ank boltgaskets next time buy cadet home depot installed per directions water supply would shut continued pouring water bowl overflow tube flow valve\n",
      "ng number go use master bathroom bought april seating area rather small water flush rear bowl consequently someone sits far back seat flush wont remo\n",
      "control inspection factory purchased months ago months age kept hearing water running one night discovered running water tank set position running ov\n",
      "tion occurrence cracked purchased two toilets fine one constant running water handle stuck thought may bad design arranged water level little higher \n",
      " twos double maybe even triple flushing give back old used extra gallon water per flush got job fine one flush percent time doesnt clean well yes wou\n",
      "quired brushing first flush second flush remove brush remnants defeated water savings would recommend product told associate lowe s would perform sto\n",
      "ittle water chair height dont like fact needs cleaned every day flushes water doesnt swirl like toilets stead swirling bubbles looks pretty darn gros\n"
     ]
    }
   ],
   "source": [
    "# find word in the text\n",
    "# select the data \n",
    "# n = int(input('Which one do you want to look at? (1-8)'))\n",
    "# textList_1 = Text(datalist[n-1]['Review'].to_string(index=False))\n",
    "textList_1 = Text(tokens[n-1])\n",
    "# textList_1.concordance(['flush'], lines=150, width=150)\n",
    "# textList_1.concordance(['american', 'standard'], lines=10, width=300)\n",
    "\n",
    "word = 'water'\n",
    "# Find the concordance of a specific word (e.g. \"Caesar\") in the text\n",
    "concordance_list = textList_1.concordance_list([word], lines=150, width=150)\n",
    "\n",
    "# Sample 20 lines from the concordance list\n",
    "sampled_lines = random.sample(concordance_list, 10)\n",
    "\n",
    "# Print the sampled lines\n",
    "print('Here is the samples of the ' + word + \":\")\n",
    "for line in sampled_lines:\n",
    "    print(line.line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ef7c14",
   "metadata": {},
   "source": [
    "#### New Method to Get Context of Key Words\n",
    "using original text of reviews and locate the full sentence, then sample them to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d23cd0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We bought the Tropic 1pc skirted toilet for ease of cleaning & it surely does not disappoint in that arena.\n",
      "The other issue is the staining below the water line, it seems the porcelain glaze is very rough and holds stains not cleaning well at all.\n",
      "We are having to constantly clean the bowl because any of the waste put into it tends to stick.\n",
      "It does not self clean.\n",
      "So you get a fairly strong flush, but the bowl doesn't get rinsed clean.\n",
      "Would be a nightmare to clean since the underside is an open grid.\n",
      "Well, waste still sticks to the bowl since it doesn't hold much water and the flush isn't enough to really clean the bowl.\n",
      "Apart from that, this model doesn't clean itself really well.\n",
      "For the first year, cleaning was almost effortless.\n",
      "They need frequent cleaning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lxf12\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer\n",
    "\n",
    "result_text_1 = datalist[n-1]['Review'].to_string(index=False)\n",
    "# result_text_1 = text[n-1]\n",
    "keyword = \"clean\"\n",
    "max_sentence_length = 150\n",
    "num_samples = 10\n",
    "\n",
    "sentences = nltk.sent_tokenize(result_text_1)\n",
    "matching_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    if keyword in sentence:\n",
    "        if len(sentence) <= max_sentence_length:\n",
    "            matching_sentences.append(sentence)\n",
    "\n",
    "if len(matching_sentences) >= num_samples:\n",
    "    samples = random.sample(matching_sentences, num_samples)\n",
    "    for sample in samples:\n",
    "        print(sample)\n",
    "else:\n",
    "    print(\"Not enough matching sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4facf",
   "metadata": {},
   "source": [
    "locate the fixed length context, then sample them to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5c420da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toilet, I does not remove paper, or waste from the FRONT of the toilet bold when flushed! I have to clean the toilet almost every time the toilet is used! The flush valve provided with this unit hangs up a\n",
      "eaves bits on the sides of the stool. In the 2 weeks we had it, we found that we had to continually clean the stool to keep it looking decent. In the end, we are obviously very disappointed.               \n",
      "                                                             \n",
      "about half the time the flush doesn't clean bowl,i would hate to have guest use.                                                               \n",
      "       \n",
      "We have had this unit in the main bathroom for about 1.5 years. We are having to constantly clean the bowl because any of the waste put into it tends to stick. It seems that the bowl was sand cast \n",
      "                                               \n",
      "We bought the Tropic 1pc skirted toilet for ease of cleaning & it surely does not disappoint in that arena. It is also low on water usage & not sure if this \n",
      " the staining below the water line, it seems the porcelain glaze is very rough and holds stains not cleaning well at all. Bad Toilet!                                                                        \n",
      "                                                                                 \n",
      "The flush doesn't clean bowl well. Uses less water but if you have to flush twice does it really?                          \n",
      "                                                                                           \n",
      "Doesn't clean well. Yes it would probably flush golf balls. We don't often try to flush them. 2 flushes are somet\n",
      " \"Change the water level or chain length\". They also state the toilet is only to remove solids, NOT clean the bowl. I replaced 3 Eljar toilets to save water, with a required brushing after the first flush \n",
      "g every 3 minutes. No amount of adjustments corrected it. I watched the video that AS suggested and cleaned out debris. When I called AS to send me another fill valve, they said it was on backorder 4-6 wee\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "result_text_1 = datalist[n-1]['Review'].to_string(index=False)\n",
    "# result_text_1 = text[n-1]\n",
    "keyword = \"clean\"\n",
    "context_length = 100\n",
    "num_samples = 10\n",
    "\n",
    "pattern = re.compile(re.escape(keyword))\n",
    "\n",
    "matches = [(match.start(), match.end()) for match in pattern.finditer(result_text_1)]\n",
    "\n",
    "matching_contexts = []\n",
    "for start, end in matches:\n",
    "    context_start = max(0, start - context_length)\n",
    "    context_end = min(len(result_text_1), end + context_length)\n",
    "    context = result_text_1[context_start:end] + result_text_1[end:context_end]\n",
    "    matching_contexts.append(context)\n",
    "\n",
    "if len(matching_contexts) >= num_samples:\n",
    "    samples = random.sample(matching_contexts, num_samples)\n",
    "    for sample in samples:\n",
    "        print(sample)\n",
    "else:\n",
    "    print(\"Not enough matching contexts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce2e92",
   "metadata": {},
   "source": [
    "#### Text Summarization on the Matching sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "231e690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_sentences = ' '.join(matching_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4d6e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37f80fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(matching_sentences)\n",
    "sentenceValue = dict()\n",
    "for sentence in sentences:\n",
    "    for word, freq in freqTable.items():\n",
    "        if word in sentence.lower():\n",
    "            if sentence in sentenceValue:\n",
    "                sentenceValue[sentence] += freq\n",
    "            else:\n",
    "                sentenceValue[sentence] = freq\n",
    "                \n",
    "sumValues = 0\n",
    "for sentence in sentenceValue:\n",
    "    sumValues += sentenceValue[sentence]\n",
    "    \n",
    "average = int(sumValues / len(sentenceValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c75e3d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Well, waste still sticks to the bowl since it doesn't hold much water and the flush isn't enough to really clean the bowl. They also state the toilet is only to remove solids, NOT clean the bowl. The other issue is the staining below the water line, it seems the porcelain glaze is very rough and holds stains not cleaning well at all. So you get a fairly strong flush, but the bowl doesn't get rinsed clean. Sometimes you need to flush a few times or use a toilet brush often, to get the bowl clean.\n"
     ]
    }
   ],
   "source": [
    "# Storing sentences into our summary.\n",
    "summary = ''\n",
    "for sentence in sentences:\n",
    "    if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
    "        summary += \" \" + sentence\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41e47d",
   "metadata": {},
   "source": [
    "#### New Methond on Text Summarization\n",
    "Compute a score for each sentence based on its importance to the overall meaning of the text. The score can be based on various criteria, such as word frequency, sentence length, or semantic similarity to other sentences in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "71c811fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, waste still sticks to the bowl since it doesn't hold much water and the flush isn't enough to really clean the bowl.\n",
      "I have to clean the toilet almost every time the toilet is used!\n",
      "The small amount of water that enters the bowl is not enough to really flush the walls of the bowl and get them clean.\n",
      "Sometimes you need to flush a few times or use a toilet brush often, to get the bowl clean.\n",
      "They need frequent cleaning.\n",
      "The flush doesn't clean bowl well.\n",
      "Doesn't clean well.\n",
      "We bought the Tropic 1pc skirted toilet for ease of cleaning & it surely does not disappoint in that arena.\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "# text = \"The quick brown fox jumps over the lazy dog. The quick brown dog jumps over the lazy cat. The quick brown cat jumps over the lazy mouse. The quick brown mouse jumps over the lazy rabbit. The quick brown rabbit jumps over the lazy deer.\"\n",
    "\n",
    "parser = PlaintextParser.from_string(matching_sentences, Tokenizer(\"english\"))\n",
    "summarizer = LexRankSummarizer()\n",
    "summary = summarizer(parser.document, sentences_count=8)\n",
    "\n",
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf9646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449de5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079507d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
